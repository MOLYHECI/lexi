dominant sequence transduction models based complex recurrent convolutional neural networks that includ encoder decoder best performing models also connect encoder decoder through attention mechanism propose simple network architecture based solely attention mechanisms dispensing with recurrence convolutions entirely machine translation tasks show these models superior quality while being more parallelizable requiring signicantly less time train model achieves BLEU toGerman translation task improving over existing best results including ensembles over BLEU EnglishtoFrench translation task model establishes singlemodel stateoftheart BLEU score after training days eight GPUs small fraction training costs best models from literature show that generalizes well other tasks applying successfully constituency parsing both with large limited training data contribution order random proposed replacing RNNs with selfattention started effort evaluate this idea with designed implemented models been crucially involved every aspect this work proposed scaled dotproduct attention multihead attention parameterfree position representation became other person involved nearly every detail designed implemented tuned evaluated countless model variants original codebase also experimented with novel model variants responsible initial codebase cient inference visualizations spent countless long days designing various parts implementing replacing earlier codebase greatly improving results massively accelerating research performed while performed while NIPS csCL neural networks long shortterm memory gated recurrent neural networks particular have been rmly established state approaches sequence modeling transduction problems such language modeling machine translation orts have since continued push boundaries recurrent language models encoderdecoder architectures models typically factor computation along symbol positions input output sequences positions steps computation time they generate sequence hidden states function previous hidden state input position inherently sequential nature precludes parallelization within training examples which becomes critical longer sequence lengths memory constraints limit batching across examp work achieved signicant improvements computational ciency through factorization tricks conditional computation while also improving model performance case latter fundamental constraint sequential computation however remains mechanisms have become integral part compelling sequence modeling transduc tion models various tasks allowing modeling dependencies without regard their distance input output sequences cases however such attention mechanisms used conjunction with recurrent network this work propose model architecture eschewing recurrence instead relying entirely attention mechanism draw global dependencies between input output allows signicantly more parallelization reach state translation quality after being trained little twelve hours eight GPUs goal reducing sequential computation also forms foundation ByteNet which convolutional neural networks basic building block computing hidden representations parallel input output positions these models number operations required relate signals from arbitrary input output positions grows distance between positions linearly logarithmically ByteNet makes more difcult learn dependencies between distant positions this reduced constant number operations albeit cost reduced effective resolution averaging attentionweighted positions effect counteract with MultiHead described section sometimes called intraattention attention mechanism relating different positions single sequence order compute representation sequence been used successfully variety tasks including reading comprehension abstractive summarization textual entailment learning taskindependent sentence representations memory networks based recurrent attention mechanism instead sequence aligned recurrence have been shown perform well simplelanguage question answering language modeling tasks best knowledge however transduction model relying entirely selfattention compute representations input output without using sequence aligned RNNs convolution following sections will describe motivate selfattention discuss advantages over models such competitive neural sequence transduction models have encoderdecoder structure encoder maps input sequence symbol representations sequence continuous representations decoder then generates output sequence symbols element time each step model autoregressive consuming previously generated symbols additional input when generating next model architecture follows this overall architecture using stacked selfattention pointwise fully connected layers both encoder decoder shown left right halves respectively encoder composed stack identical layers layer sublayers multihead selfattention mechanism second simple position wise fully connected feedforward network employ residual connection around each sublayers followed layer normalization output each sublayer LayerNorm where function implemented sublayer itself facilitate these residual connections blayers model well embedding layers produce outputs dimension model decoder also composed stack identical layers addition sublayers each encoder layer decoder inserts third sublayer which performs multihead attention over output encoder stack encoder employ residual connections around each sublayers followed layer normalization also modify selfattention sublayer decoder stack prevent positions from attending subsequent positions masking combined with fact that output embeddings offset position ensures that predictions position depend only known outputs positions less than attention function described mapping query keyvalue pairs output where query keys values output vectors output computed weighted DotProduct MultiHead left DotProduct right MultiHead consists several attention layers running parallel values where weight assigned each value computed compatibility function query with corresponding DotProduct call particular attention DotProduct input consists queries keys dimension values dimension compute products query with keys divide each apply softmax function obtain weights values practice compute attention function queries simultaneously packed together into matrix keys values also packed together into matrices compute matrix outputs oftm most commonly used attention functions additive attention dotproduct multi plicative attention attention identical algorithm except scaling factor attention computes compatibility function using feedforward network with single hidden layer similar theoretical complexity dotproduct attention much faster more spaceef cient practice since implemented using highly optimized matrix multiplication code small values mechanisms perform similarly additive attention outperforms product attention without scaling larger values suspect that large values products grow large magnitu pushing softmax function into regions where extremely small gradients counteract this effect scale products MultiHead performing single attention function with model dimensional keys values queries found benecial linearly project queries keys values times with different learned linear projections dimensions respectively each these projected versions queries keys values then perform attention function parallel yielding dimensional illustrate products large assume that components independent random variables with mean variance their product mean variance output values concatenated once again projected resulting values depicted attention allows model jointly attend information from different representation subspaces different positions single attention head averaging inhibits this MultiHead head where head projections parameter matrices model model model model this work employ parallel attention layers heads each these model reduced dimension each head total computational cost similar that singlehead attention with full dimensionality uses multihead attention three different ways encoderdecoder attention layers queries come from previous decoder layer memory keys values come from output encoder allows every position decoder attend over positions input sequence mimics typical encoderdecoder attention mechanisms sequencetosequence models such encoder contains selfattention layers selfattention layer keys values queries come from same place this case output previous layer encoder position encoder attend positions previous layer encoder selfattention layers decoder allow each position decoder attend positions decoder including that position need prevent leftward information decoder preserve autoregressive property implement this inside scaled dotproduct attention masking setting values input softmax which correspond illegal connections FeedForward addition attention sublayers each layers encoder decoder contains fully connected feedforward network which applied each position separately identically consists linear transformations with ReLU activation between linear transformations same across different positions they diff erent parameters from layer layer describing this convolutions with kernel size dimensionality input output model innerlayer dimensionality other sequence transduction models learned embeddings convert input tokens output tokens vectors dimension model also usual learned linear transfor mation softmax function convert decoder output predicted nexttoken probabilities model share same weight matrix between embedding layers presoftmax linear transformation similar embedding layers multiply those weights model path lengths perlayer complexity minimum number sequential operations different layer types sequence length representation dimension kernel size convolutions size neighborhood restricted selfattention SelfAttention SelfAttention restricted model contains recurrence convolution order model make order sequence must inject some information about relative absolute position tokens sequence this positional encodings input embeddings bottoms encoder decoder stacks positional encodings have same dimension model embeddings that summed many choices positional encodings learned this work sine cosine functions different frequencies model model where position dimension each dimension positional encoding corresponds sinusoid wavelengths form geometric progression chose this function because hypothesized would allow model easily learn attend relative positions since offset represented linear function also experimented with using learned positional embeddings instead found that versions produced nearly identical results chose sinusoidal version because allow model extrapolate sequence lengths longer than ones encountered during training SelfAttention this section compare various aspects selfattention layers recurrent convolu tional layers commonly used mapping variablelength sequence symbol representations another sequence equal length with such hidden layer typical sequence transduction encoder decoder selfattention consider three desiderata total computational complexity layer amount computation that parallelized measured minimum number sequential operations required third path length between longrange dependencies network longrange dependencies challenge many sequence transduction tasks factor affecting ability learn such dependencies length paths forward backward signals have traverse netwo shorter these paths between combination positions input outpu sequences easier learn longrange dependencies also compare maximum path length between input output positions networks composed different layer types noted selfattention layer connects positions with constant number sequentially executed operations whereas recurrent layer requires sequential operations terms computational complexity selfattention layers faster than recurrent layers when sequence length smaller than representation dimensionality which most often case with sentence representations used stateoftheart models machine translations such wordpiece bytepair representations improve computational performance tasks involving very long sequences selfattention could restricted considering only neighborhood size input sequence centered around respective output position would increase maximum path length plan investigate this approach further future work single convolutional layer with kernel width does connect pairs input output positions requires stack convolutional layers case contiguous kernels case dilated convolutions increasing length longest paths between positions network layers generally more expensive than recurrent layers factor convolutions however decrease complexity considerably with however complexity separable convolution equal combination selfattention layer pointwise feedforward layer approach take model side benet selfattention could yield more interpretable models inspect attention distributions from models present discuss examples appendix only individual attention heads clearly learn perform diff erent tasks many appear exhibit behavior related syntactic semantic structure sentences section describes training regime models trained standard EnglishGerman dataset consisting about million sentence pairs were encoded using bytepair encoding which shared source target vocabulary about tokens EnglishFrench used signicantly larger EnglishFrench dataset consisting sentences split tokens into wordpiece vocabulary pairs were batched together approximate sequence length training batch contained sentence pairs containing approximately source tokens target tokens trained models machine with NVIDIA GPUs base models using hyperparameters described throughout paper each training step took about seconds trained base models total steps hours modelsdescribed bottom line table step time seconds models were trained steps days used optimizer with varied learning rate over course training according formula model step step steps corresponds increasing learning rate linearly steps training steps decreasing thereafter proportionally inverse square root step number used steps employ three types regularization during training achieves better BLEU scores than previous stateoftheart models EnglishtoGerman EnglishtoFrench tests fraction training cost BLEU FLOPs ENDE ENFR ENDE ENFR ByteNet DeepAtt PosUnk GNMT DeepAtt PosUnk GNMT base model apply dropout output each sublayer before added sublayer input normalized addition apply dropout sums embeddings positional encodings both encoder decoder stacks base model rate training employed label smoothing value hurts perplexity model learns more unsure improves accuracy BLEU score EnglishtoGerman translation task transformer model outperforms best previously reported models including ensembles more than BLEU establishing stateoftheart BLEU score conguration this model listed bottom line took days GPUs base model surpasses previously published models ensembles fraction training cost competitive models EnglishtoFrench translation task model achieves BLEU score outperforming previously published single models less than training cost previous stateoftheart model model trained EnglishtoFrench used dropout rate instead base models used single model obtained averaging last checkpoints which were written intervals models averaged last checkpoints used beam search with beam size length penalty hyperparameters were chosen after experimentation development maximum output length during inference input length terminate early when possible summarizes results compares translation quality training costs other model architectures from literature estimate number oating point operations used train model multiplying training time number GPUs used estimate sustained singleprecision oatingpoint capacity each evaluate importance different components varied base model different ways measuring change performance EnglishtoGerman translation used values TFLOPS respectively architecture values identical those base model metrics EnglishtoGerman translation development perplexities perwordpiece according bytepair encoding should compared perword perplexities model train BLEU params steps base positional embedding instead sinusoids development used beam search described previous section checkpoint averaging present these results rows vary number attention heads attention value dimensions keeping amount computation constant described singlehead attention BLEU worse than best setting quality also drops with many heads rows observe that reducing attention size hurts model quality suggests that determining compatibility easy that more sophisticated compatibility function than product benecial further observe rows that expected bigger models better dropout very helpful avoiding overtting replace sinusoidal positional encoding with learned positional embeddings observe nearly identical results base model evaluate generalize other tasks performed experiments constituency parsing task presents specic challenges output subject strong structural constraints signicantly longer than input sequencetosequence models have been able attain stateoftheart results smalldata regimes trained transformer with model portion about training sentences also trained semisupervised setting using larger highcondence BerkleyParser corpora from with approximately sentences used vocabulary tokens only setting vocabulary tokens semisupervised setting performed only small number experiments select dropout both attention residual section learning rates beam size development other parameters remained unchanged from EnglishtoGerman base translation model inference generalizes well constituency parsing only discriminative only discriminative only discriminative only discriminative layers only discriminative semisupervised semisupervised McClosky semisupervised semisupervised layers semisupervised multitask generative increased maximum output length input length used beam size both only semisupervised setting results show that despite lack taskspecic tuning model performs prisingly well yielding better results than previously reported models with exception contrast sequencetosequence models outperforms even when training only training sentences this work presented sequence transduction model based entirely attention replacing recurrent layers most commonly used encoderdecoder architectures with multiheaded selfattention translation tasks trained signicantly faster than architectures based recurrent convolutional layers both EnglishtoGerman EnglishtoFrench translation tasks achieve state former task best model outperforms even previously reported ensembles excited about future attentionbased models plan apply them other tasks plan extend problems involving input output modalities other than text investigate local restricted attention mechanisms efciently handle large inputs outputs such images audio video generation less sequential another research goals ours code used train evaluate models available httpsgithubcom grateful their fruitful comments corrections inspiration normalization arXiv preprint machine translation jointly learning align translate CoRR MinhThang exploration neural machine translation architectures CoRR shortterm memorynetworks machine reading arXiv preprint phrase representations using encoderdecoder statistical machine translation CoRR learning with depthwise separable convolutions arXiv preprint evaluation gated recurrent neural networks sequence modeling CoRR neural network grammars NAACL tional sequence sequence learning arXiv preprint sequences with recurrent neural networks arXiv preprint residual learning recognition IEEE pages recurrent nets difculty learning longterm dependencies shortterm memory computation PCFG grammars with latent annotations across languages pages limits language modeling arXiv preprint ukasz active memory replace attention NIPS ukasz GPUs learn algorithms ICLR machine translation linear time arXiv preprint attention networks method stochastic optimization ICLR tricks LSTM networks arXiv preprint structured selfattentive sentence embedding arXiv preprint MinhThang sequence sequence learning arXiv preprint MinhThang ective approaches attention based neural machine translation arXiv preprint large annotated corpus english penn treebank linguistics McClosky selftraining parsing NAACL pages decomposable attention model deep reinforced model abstractive summarization arXiv preprint accurate compact interpretable tree annotation pages output embedding improve language models arXiv preprint machine translation rare words with subword units arXiv preprint large neural networks sparselygated mixtureofexperts layer arXiv preprint simple prevent neural networks from overtting memory networks editors pages sequence learning with neural networks pages inception architecture computer vision CoRR foreign language neural machine translation system between human machine translation arXiv preprint recurrent models with fastforward connections neural machine translation CoRR accurate shiftreduce constituent parsing pages example attention mechanism following longdistance dependencies encoder selfattention layer attention heads attend distant dependency verb making completing phrase makingmore difcult here shown only word making colors represent different heads viewed color attention heads also layer apparently involved anaphora resolution attentions head attentions from just word attention heads that attentions very sharp this word attention heads exhibit behaviour that seems related structure sentence give such examples above from diff erent heads from encoder selfattention layer heads clearly learned perform different tasks