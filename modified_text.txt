 Attention Need Abstract dominant sequence transduction models based complex recurrent convolutional neural networks that includ encoder a decoder best performing models also connect encoder decoder through attention mechanism propose a simple network architecture Transformer based solely attention mechanisms dispensing with recurrence convolutions entirely Experiments machine translation tasks show these models superior quality while being more parallelizable requiring signicantly less time train model achieves BLEU 2014 English toGerman translation task improving over existing best results including ensembles over BLEU 2014 EnglishtoFrench translation task model establishes a singlemodel stateoftheart BLEU score after training days eight GPUs a small fraction training costs best models from literature show that Transformer generalizes well other tasks applying successfully English constituency parsing both with large limited training data Equal contribution Listing order random Jakob proposed replacing RNNs with selfattention started effort evaluate this idea Ashish with Illia designed implemented Transformer models been crucially involved every aspect this work Noam proposed scaled dotproduct attention multihead attention parameterfree position representation became other person involved nearly every detail Niki designed implemented tuned evaluated countless model variants original codebase tensor2tensor Llion also experimented with novel model variants responsible initial codebase cient inference visualizations Lukasz Aidan spent countless long days designing various parts implementing tensor2tensor replacing earlier codebase greatly improving results massively accelerating research Work performed while Google Brain Work performed while Google Research 31st Conference Neural Information Processing Systems NIPS 2017 Long Beach arXiv170603762v7 csCL 2023 Introduction Recurrent neural networks long shortterm memory gated recurrent neural networks particular have been rmly established state approaches sequence modeling transduction problems such language modeling machine translation Numerous orts have since continued push boundaries recurrent language models encoderdecoder architectures Recurrent models typically factor computation along symbol positions input output sequences Aligning positions steps computation time they generate a sequence hidden states a function previous hidden state input position This inherently sequential nature precludes parallelization within training examples which becomes critical longer sequence lengths memory constraints limit batching across examp Recent work achieved signicant improvements computational ciency through factorization tricks conditional computation while also improving model performance case latter fundamental constraint sequential computation however remains Attention mechanisms have become integral part compelling sequence modeling transduc tion models various tasks allowing modeling dependencies without regard their distance input output sequences a cases however such attention mechanisms used conjunction with a recurrent network this work propose Transformer a model architecture eschewing recurrence instead relying entirely attention mechanism draw global dependencies between input output Transformer allows signicantly more parallelization reach a state translation quality after being trained little twelve hours eight P100 GPUs Background goal reducing sequential computation also forms foundation Extended Neural ByteNet ConvS2S which convolutional neural networks basic building block computing hidden representations parallel input output positions these models number operations required relate signals from arbitrary input output positions grows distance between positions linearly ConvS2S logarithmically ByteNet This makes more difcult learn dependencies between distant positions Transformer this reduced a constant number operations albeit cost reduced effective resolution averaging attentionweighted positions effect counteract with MultiHead Attention described section Selfattention sometimes called intraattention attention mechanism relating different positions a single sequence order compute a representation sequence Selfattention been used successfully a variety tasks including reading comprehension abstractive summarization textual entailment learning taskindependent sentence representations Endtoend memory networks based a recurrent attention mechanism instead sequence aligned recurrence have been shown perform well simplelanguage question answering language modeling tasks best knowledge however Transformer transduction model relying entirely selfattention compute representations input output without using sequence aligned RNNs convolution following sections will describe Transformer motivate selfattention discuss advantages over models such Model Architecture Most competitive neural sequence transduction models have encoderdecoder structure Here encoder maps input sequence symbol representations a sequence continuous representations Given decoder then generates output sequence symbols element a time each step model autoregressive consuming previously generated symbols additional input when generating next Figure Transformer model architecture Transformer follows this overall architecture using stacked selfattention pointwise fully connected layers both encoder decoder shown left right halves Figure respectively Encoder Decoder Stacks Encoder encoder composed a stack identical layers Each layer sublayers a multihead selfattention mechanism second a simple position wise fully connected feedforward network employ a residual connection around each sublayers followed layer normalization That output each sublayer LayerNorm Sublayer where Sublayer function implemented sublayer itself facilitate these residual connections blayers model well embedding layers produce outputs dimension model Decoder decoder also composed a stack identical layers addition sublayers each encoder layer decoder inserts a third sublayer which performs multihead attention over output encoder stack Similar encoder employ residual connections around each sublayers followed layer normalization also modify selfattention sublayer decoder stack prevent positions from attending subsequent positions This masking combined with fact that output embeddings offset position ensures that predictions position depend only known outputs positions less than Attention attention function described mapping a query a keyvalue pairs output where query keys values output vectors output computed a weighted Scaled DotProduct Attention MultiHead Attention Figure left Scaled DotProduct Attention right MultiHead Attention consists several attention layers running parallel values where weight assigned each value computed a compatibility function query with corresponding Scaled DotProduct Attention call particular attention Scaled DotProduct Attention Figure input consists queries keys dimension values dimension compute products query with keys divide each apply a softmax function obtain weights values practice compute attention function a queries simultaneously packed together into a matrix keys values also packed together into matrices compute matrix outputs Attention oftm most commonly used attention functions additive attention dotproduct multi plicative attention Dotproduct attention identical algorithm except scaling factor Additive attention computes compatibility function using a feedforward network with a single hidden layer While similar theoretical complexity dotproduct attention much faster more spaceef cient practice since implemented using highly optimized matrix multiplication code While small values mechanisms perform similarly additive attention outperforms product attention without scaling larger values suspect that large values products grow large magnitu pushing softmax function into regions where extremely small gradients counteract this effect scale products MultiHead Attention Instead performing a single attention function with model dimensional keys values queries found benecial linearly project queries keys values times with different learned linear projections dimensions respectively each these projected versions queries keys values then perform attention function parallel yielding dimensional illustrate products large assume that components independent random variables with mean variance Then their product mean variance output values These concatenated once again projected resulting values depicted Figure Multihead attention allows model jointly attend information from different representation subspaces different positions With a single attention head averaging inhibits this MultiHead Concathead head where head Attention Where projections parameter matrices model model model model this work employ parallel attention layers heads each these model reduced dimension each head total computational cost similar that singlehead attention with full dimensionality Applications Attention Model Transformer uses multihead attention three different ways encoderdecoder attention layers queries come from previous decoder layer memory keys values come from output encoder This allows every position decoder attend over positions input sequence This mimics typical encoderdecoder attention mechanisms sequencetosequence models such encoder contains selfattention layers a selfattention layer keys values queries come from same place this case output previous layer encoder Each position encoder attend positions previous layer encoder Similarly selfattention layers decoder allow each position decoder attend positions decoder including that position need prevent leftward information decoder preserve autoregressive property implement this inside scaled dotproduct attention masking setting values input softmax which correspond illegal connections Figure Positionwise FeedForward Networks addition attention sublayers each layers encoder decoder contains a fully connected feedforward network which applied each position separately identically This consists linear transformations with a ReLU activation between max0 While linear transformations same across different positions they diff erent parameters from layer layer Another describing this convolutions with kernel size dimensionality input output model innerlayer dimensionality 2048 Embeddings Softmax Similarly other sequence transduction models learned embeddings convert input tokens output tokens vectors dimension model also usual learned linear transfor mation softmax function convert decoder output predicted nexttoken probabilities model share same weight matrix between embedding layers presoftmax linear transformation similar embedding layers multiply those weights model Table Maximum path lengths perlayer complexity minimum number sequential operations different layer types sequence length representation dimension kernel size convolutions size neighborhood restricted selfattention Layer Type Complexity Layer Sequential Maximum Path Length Operations SelfAttention Recurrent Convolutional SelfAttention restricted Positional Encoding Since model contains recurrence convolution order model make order sequence must inject some information about relative absolute position tokens sequence this positional encodings input embeddings bottoms encoder decoder stacks positional encodings have same dimension model embeddings that summed There many choices positional encodings learned this work sine cosine functions different frequencies 10000 model 10000 model where position dimension That each dimension positional encoding corresponds a sinusoid wavelengths form a geometric progression 10000 chose this function because hypothesized would allow model easily learn attend relative positions since offset represented a linear function also experimented with using learned positional embeddings instead found that versions produced nearly identical results Table chose sinusoidal version because allow model extrapolate sequence lengths longer than ones encountered during training SelfAttention this section compare various aspects selfattention layers recurrent convolu tional layers commonly used mapping variablelength sequence symbol representations another sequence equal length with such a hidden layer a typical sequence transduction encoder decoder Motivating selfattention consider three desiderata total computational complexity layer Another amount computation that parallelized measured minimum number sequential operations required third path length between longrange dependencies network Learning longrange dependencies a challenge many sequence transduction tasks factor affecting ability learn such dependencies length paths forward backward signals have traverse netwo shorter these paths between combination positions input outpu sequences easier learn longrange dependencies Hence also compare maximum path length between input output positions networks composed different layer types noted Table a selfattention layer connects positions with a constant number sequentially executed operations whereas a recurrent layer requires sequential operations terms computational complexity selfattention layers faster than recurrent layers when sequence length smaller than representation dimensionality which most often case with sentence representations used stateoftheart models machine translations such wordpiece bytepair representations improve computational performance tasks involving very long sequences selfattention could restricted considering only a neighborhood size input sequence centered around respective output position This would increase maximum path length plan investigate this approach further future work single convolutional layer with kernel width does connect pairs input output positions Doing requires a stack convolutional layers case contiguous kernels case dilated convolutions increasing length longest paths between positions network Convolutional layers generally more expensive than recurrent layers a factor Separable convolutions however decrease complexity considerably Even with however complexity a separable convolution equal combination a selfattention layer a pointwise feedforward layer approach take model side benet selfattention could yield more interpretable models inspect attention distributions from models present discuss examples appendix only individual attention heads clearly learn perform diff erent tasks many appear exhibit behavior related syntactic semantic structure sentences Training This section describes training regime models Training Data Batching trained standard 2014 EnglishGerman dataset consisting about million sentence pairs Sentences were encoded using bytepair encoding which a shared source target vocabulary about 37000 tokens EnglishFrench used signicantly larger 2014 EnglishFrench dataset consisting sentences split tokens into a 32000 wordpiece vocabulary Sentence pairs were batched together approximate sequence length Each training batch contained a sentence pairs containing approximately 25000 source tokens 25000 target tokens Hardware Schedule trained models machine with NVIDIA P100 GPUs base models using hyperparameters described throughout paper each training step took about seconds trained base models a total 100000 steps hours modelsdescribed bottom line table step time seconds models were trained 300000 steps days Optimizer used Adam optimizer with varied learning rate over course training according formula model step step steps This corresponds increasing learning rate linearly steps training steps decreasing thereafter proportionally inverse square root step number used steps 4000 Regularization employ three types regularization during training Table Transformer achieves better BLEU scores than previous stateoftheart models EnglishtoGerman EnglishtoFrench newstest2014 tests a fraction training cost Model BLEU Training Cost FLOPs ENDE ENFR ENDE ENFR ByteNet 2375 DeepAtt PosUnk GNMT 3992 ConvS2S 2516 4046 2603 4056 DeepAtt PosUnk Ensemble GNMT Ensemble 2630 4116 ConvS2S Ensemble 2636 4129 Transformer base model Transformer Residual Dropout apply dropout output each sublayer before added sublayer input normalized addition apply dropout sums embeddings positional encodings both encoder decoder stacks base model a rate Label Smoothing During training employed label smoothing value This hurts perplexity model learns more unsure improves accuracy BLEU score Results Machine Translation 2014 EnglishtoGerman translation task transformer model Transformer Table outperforms best previously reported models including ensembles more than BLEU establishing a stateoftheart BLEU score conguration this model listed bottom line Table Training took days P100 GPUs Even base model surpasses previously published models ensembles a fraction training cost competitive models 2014 EnglishtoFrench translation task model achieves a BLEU score outperforming previously published single models less than training cost previous stateoftheart model Transformer model trained EnglishtoFrench used dropout rate instead base models used a single model obtained averaging last checkpoints which were written 10minute intervals models averaged last checkpoints used beam search with a beam size length penalty These hyperparameters were chosen after experimentation development maximum output length during inference input length terminate early when possible Table summarizes results compares translation quality training costs other model architectures from literature estimate number oating point operations used train a model multiplying training time number GPUs used estimate sustained singleprecision oatingpoint capacity each Model Variations evaluate importance different components Transformer varied base model different ways measuring change performance EnglishtoGerman translation used values TFLOPS P100 respectively Table Variations Transformer architecture Unlisted values identical those base model metrics EnglishtoGerman translation development newstest2013 Listed perplexities perwordpiece according bytepair encoding should compared perword perplexities model train BLEU params steps base 2048 100K 1024 1024 4096 positional embedding instead sinusoids 1024 4096 300K development newstest2013 used beam search described previous section checkpoint averaging present these results Table Table rows vary number attention heads attention value dimensions keeping amount computation constant described Section While singlehead attention BLEU worse than best setting quality also drops with many heads Table rows observe that reducing attention size hurts model quality This suggests that determining compatibility easy that a more sophisticated compatibility function than product benecial further observe rows that expected bigger models better dropout very helpful avoiding overtting replace sinusoidal positional encoding with learned positional embeddings observe nearly identical results base model English Constituency Parsing evaluate Transformer generalize other tasks performed experiments English constituency parsing This task presents specic challenges output subject strong structural constraints signicantly longer than input Furthermore sequencetosequence models have been able attain stateoftheart results smalldata regimes trained a 4layer transformer with model 1024 Wall Street Journal portion Penn Treebank about training sentences also trained a semisupervised setting using larger highcondence BerkleyParser corpora from with approximately sentences used a vocabulary tokens only setting a vocabulary tokens semisupervised setting performed only a small number experiments select dropout both attention residual section learning rates beam size Section development other parameters remained unchanged from EnglishtoGerman base translation model During inference Table Transformer generalizes well English constituency parsing Results Section Parser Training Vinyals Kaiser 2014 only discriminative Petrov 2006 only discriminative 2013 only discriminative Dyer 2016 only discriminative Transformer layers only discriminative 2013 semisupervised Huang Harper 2009 semisupervised McClosky 2006 semisupervised Vinyals Kaiser 2014 semisupervised Transformer layers semisupervised Luong 2015 multitask Dyer 2016 generative increased maximum output length input length used a beam size both only semisupervised setting results Table show that despite lack taskspecic tuning model performs prisingly well yielding better results than previously reported models with exception Recurrent Neural Network Grammar contrast sequencetosequence models Transformer outperforms Berkeley Parser even when training only training sentences Conclusion this work presented Transformer sequence transduction model based entirely attention replacing recurrent layers most commonly used encoderdecoder architectures with multiheaded selfattention translation tasks Transformer trained signicantly faster than architectures based recurrent convolutional layers both 2014 EnglishtoGerman 2014 EnglishtoFrench translation tasks achieve a state former task best model outperforms even previously reported ensembles excited about future attentionbased models plan apply them other tasks plan extend Transformer problems involving input output modalities other than text investigate local restricted attention mechanisms efciently handle large inputs outputs such images audio video Making generation less sequential another research goals ours code used train evaluate models available httpsgithubcom tensorflowtensor2tensor Acknowledgements grateful Kalchbrenner Stephan Gouws their fruitful comments corrections inspiration References Jimmy Jamie Ryan Kiros Geoffrey Hinton Layer normalization arXiv preprint arXiv160706450 2016 Dzmitry Bahdanau Kyunghyun Yoshua Bengio Neural machine translation jointly learning align translate CoRR abs14090473 2014 Denny Britz Anna Goldie MinhThang Luong Quoc Massive exploration neural machine translation architectures CoRR abs170303906 2017 Jianpeng Cheng Dong Mirella Lapata Long shortterm memorynetworks machine reading arXiv preprint arXiv160106733 2016 Kyunghyun Bart Merrienboer Caglar Gulcehre Fethi Bougares Holger Schwenk Yoshua Bengio Learning phrase representations using encoderdecoder statistical machine translation CoRR abs14061078 2014 Francois Chollet Xception Deep learning with depthwise separable convolutions arXiv preprint arXiv161002357 2016 Junyoung Chung Glehre Kyunghyun Yoshua Bengio Empirical evaluation gated recurrent neural networks sequence modeling CoRR abs14123555 2014 Chris Dyer Adhiguna Kuncoro Miguel Ballesteros Noah Smith Recurrent neural network grammars Proc NAACL 2016 Jonas Gehring Michael Auli David Grangier Denis Yarats Yann Dauphin Convolu tional sequence sequence learning arXiv preprint arXiv170503122v2 2017 Alex Graves Generating sequences with recurrent neural networks arXiv preprint arXiv13080850 2013 Kaiming Xiangyu Zhang Shaoqing Jian Deep residual learning recognition Proceedings IEEE Conference Computer Vision Pattern Recognition pages 770778 2016 Sepp Hochreiter Yoshua Bengio Paolo Frasconi Jrgen Schmidhuber Gradient recurrent nets difculty learning longterm dependencies 2001 Sepp Hochreiter Jrgen Schmidhuber Long shortterm memory Neural computation 9817351780 1997 Zhongqiang Huang Mary Harper Selftraining PCFG grammars with latent annotations across languages Proceedings 2009 Conference Empirical Methods Natural Language Processing pages 832841 August 2009 Rafal Jozefowicz Oriol Vinyals Mike Schuster Noam Shazeer Yonghui Exploring limits language modeling arXiv preprint arXiv160202410 2016 ukasz Kaiser Samy Bengio active memory replace attention Advances Neural Information Processing Systems NIPS 2016 ukasz Kaiser Ilya Sutskever Neural GPUs learn algorithms International Conference Learning Representations ICLR 2016 Kalchbrenner Lasse Espeholt Karen Simonyan Aaron Oord Alex Graves Kavukcuoglu Neural machine translation linear time arXiv preprint arXiv161010099v2 2017 Yoon Carl Denton Luong Hoang Alexander Rush Structured attention networks International Conference Learning Representations 2017 Diederik Kingma Jimmy Adam method stochastic optimization ICLR 2015 Oleksii Kuchaiev Boris Ginsburg Factorization tricks LSTM networks arXiv preprint arXiv170310722 2017 Zhouhan Minwei Feng Cicero Nogueira Santos Bing Xiang Bowen Zhou Yoshua Bengio structured selfattentive sentence embedding arXiv preprint arXiv170303130 2017 MinhThang Luong Quoc Ilya Sutskever Oriol Vinyals Lukasz Kaiser Multitask sequence sequence learning arXiv preprint arXiv151106114 2015 MinhThang Luong Hieu Pham Christopher Manning ective approaches attention based neural machine translation arXiv preprint arXiv150804025 2015 Mitchell Marcus Mary Marcinkiewicz Beatrice Santorini Building a large annotated corpus english penn treebank Computational linguistics 192313330 1993 David McClosky Eugene Charniak Mark Johnson Effective selftraining parsing Proceedings Human Language Technology Conference NAACL Main Conference pages 152159 June 2006 Ankur Parikh Oscar Tckstrm Dipanjan Jakob Uszkoreit decomposable attention model Empirical Methods Natural Language Processing 2016 Romain Paulus Caiming Xiong Richard Socher deep reinforced model abstractive summarization arXiv preprint arXiv170504304 2017 Slav Petrov Leon Barrett Romain Thibaux Klein Learning accurate compact interpretable tree annotation Proceedings 21st International Conference Computational Linguistics 44th Annual Meeting pages 433440 July 2006 Press Lior Wolf Using output embedding improve language models arXiv preprint arXiv160805859 2016 Rico Sennrich Barry Haddow Alexandra Birch Neural machine translation rare words with subword units arXiv preprint arXiv150807909 2015 Noam Shazeer Azalia Mirhoseini Krzysztof Maziarz Andy Davis Quoc Geoffrey Hinton Jeff Dean Outrageously large neural networks sparselygated mixtureofexperts layer arXiv preprint arXiv170106538 2017 Nitish Srivastava Geoffrey Hinton Alex Krizhevsky Ilya Sutskever Ruslan Salakhutdi Dropout a simple prevent neural networks from overtting Journal Machine Learning Research 15119291958 2014 Sainbayar Sukhbaatar Arthur Szlam Jason Weston Fergus Endtoend memory networks Cortes Lawrence Sugiyama Garnett editors Advances Neural Information Processing Systems pages 24402448 Curran Associates 2015 Ilya Sutskever Oriol Vinyals Quoc Sequence sequence learning with neural networks Advances Neural Information Processing Systems pages 31043112 2014 Christian Szegedy Vincent Vanhoucke Sergey Ioffe Jonathon Shlens Zbigniew Wojna Rethinking inception architecture computer vision CoRR abs151200567 2015 Vinyals Kaiser Petrov Sutskever Hinton Grammar a foreign language Advances Neural Information Processing Systems 2015 Yonghui Mike Schuster Zhifeng Chen Quoc Mohammad Norouzi Wolfgang Macherey Maxim Krikun Yuan Klaus Macherey Googles neural machine translation system Bridging between human machine translation arXiv preprint arXiv160908144 2016 Zhou Ying Xuguang Wang Peng Deep recurrent models with fastforward connections neural machine translation CoRR abs160604199 2016 Muhua Zhang Wenliang Chen Zhang Jingbo Fast accurate shiftreduce constituent parsing Proceedings 51st Annual Meeting Volume Long Papers pages 434443 August 2013 Attention Visualizations Figure example attention mechanism following longdistance dependencies encoder selfattention layer Many attention heads attend a distant dependency verb making completing phrase makingmore difcult Attentions here shown only word making Different colors represent different heads Best viewed color Figure attention heads also layer apparently involved anaphora resolution Full attentions head Bottom Isolated attentions from just word attention heads Note that attentions very sharp this word Figure Many attention heads exhibit behaviour that seems related structure sentence give such examples above from diff erent heads from encoder selfattention layer heads clearly learned perform different tasks